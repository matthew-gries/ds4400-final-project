{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4400_final_project.dataset.constants import DATASET_FOLDER\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(csv_filename: str) -> Tuple[np.array, np.array, Dict[int, str], Dict[str, int]]:\n",
    "\t\"\"\" Load the CSV file from the dataset folder. \"\"\"\n",
    "\tfile = str(Path(DATASET_FOLDER) / csv_filename)\n",
    "\tfeatures_list = np.genfromtxt(file, dtype=None, encoding=None, delimiter=\",\", skip_header=1, usecols=range(2, 60))\n",
    "\tfeatures = np.array([list(x) for x in features_list])\n",
    "\n",
    "\t# Create a mapping between a numeric value and genre\n",
    "\tindex_genre_map = {i: genre for i, genre in enumerate(np.unique(features[:,-1]))}\n",
    "\tgenre_index_map = {value: key for key, value in index_genre_map.items()}\n",
    "\n",
    "\t# split the inputs and their labels\n",
    "\tx = normalize(features[:,:57])\n",
    "\t# y = normalize(np.array([genre_index_map[genre] for genre in features[:,-1]]).reshape(-1, 1))\n",
    "\n",
    "\t# y = np.array([make_one_hot(genre_index_map[genre]) for genre in features[:,-1]])\n",
    "\ty = np.array([genre_index_map[genre] for genre in features[:,-1]])\n",
    "\n",
    "\treturn x, y, index_genre_map, genre_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data from the 3 seconds and 30 seconds features CSV\n",
    "X_3, y_3, index_genre_map_3, genre_index_map_3 = load_data_from_file(\"features_3_sec.csv\")\n",
    "X_30, y_30, index_genre_map_30, genre_index_map_30 = load_data_from_file(\"features_30_sec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all the data into training and testing sets\n",
    "TEST_SIZE = 0.33\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "x_3_train, x_3_test, y_3_train, y_3_test = train_test_split(X_3, y_3, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "x_30_train, x_30_test, y_30_train, y_30_test = train_test_split(X_30, y_30, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "x_3_train, x_3_test, x_30_train, x_30_test = map(torch.FloatTensor, [x_3_train, x_3_test, x_30_train, x_30_test])\n",
    "y_3_train, y_3_test, y_30_train, y_30_test = map(torch.LongTensor, [y_3_train, y_3_test, y_30_train, y_30_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden: int):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(57, num_hidden)\n",
    "        self.output = nn.Linear(num_hidden, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModelTwoHiddenLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden_nodes_1: int, num_hidden_nodes_2: int):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(57, num_hidden_nodes_1)\n",
    "        self.hidden2 = nn.Linear(num_hidden_nodes_1, num_hidden_nodes_2)\n",
    "        self.output = nn.Linear(num_hidden_nodes_2, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self._x_data = x_data\n",
    "        self._y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return [self._x_data[idx], self._y_data[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_func, x_train, y_train, epochs: int, lr: float, l2_reg: float, batch_size: int, verbose = False):\n",
    "\tdataset = GTZANDataset(x_train, y_train)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\toptimizer = torch.optim.SGD(params=model.parameters(), lr=lr, weight_decay=l2_reg)\n",
    "\n",
    "\t# set the model to training mode\n",
    "\tmodel.train()\n",
    "\n",
    "\t# save average losses every epoch\n",
    "\tlosses = []\n",
    "\n",
    "\t# optimize the model\n",
    "\tfor epoch in range(epochs):\n",
    "\n",
    "\t\tprint(f\"\\nEpoch {epoch+1}\\n-------------------------------\") if verbose else None\n",
    "\n",
    "\t\tlosses_each_batch = []\n",
    "\n",
    "\t\tfor batch, (x, y) in enumerate(dataloader):\n",
    "\n",
    "\t\t\ty_pred = model(x)\n",
    "\n",
    "\t\t\tloss = loss_func(y_pred, y)\n",
    "\n",
    "\t\t\t# backpropogation\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# save the loss for this batch\n",
    "\t\t\tlosses_each_batch.append(loss.item())\n",
    "\n",
    "\t\t\tif verbose and batch % (batch_size // 10) == 0:\n",
    "\t\t\t\tavg_running_loss = sum(losses_each_batch) / len(losses_each_batch)\n",
    "\t\t\t\tcurrent = batch * len(x)\n",
    "\t\t\t\tprint(f\"Epoch [{epoch:03}] Batch [{batch:03}] avg_running_loss: {avg_running_loss:>7f} [{current:>5d}/{len(dataset):>5d}]\")\n",
    "\n",
    "\t\t# average all the batch losses for this epoch and save it\n",
    "\t\tavg_loss_epoch = sum(losses_each_batch) / len(losses_each_batch)\n",
    "\t\tlosses.append(avg_loss_epoch)\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"Epoch [{epoch:03}]: average loss: {avg_loss_epoch:>7f}\")\n",
    "\n",
    "\t# set the model to evaluation mode\n",
    "\tmodel.eval()\n",
    "\treturn model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_experiments(experiment_configs, x_train, y_train):\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    for model_id, config in experiment_configs.items():\n",
    "        model_class = config[\"model_class\"]\n",
    "        model_kwargs = config[\"model_kwargs\"] if \"model_kwargs\" in config.keys() else {} \n",
    "        model_instance = model_class(**model_kwargs)\n",
    "        loss_func_class = config[\"loss_func_class\"]\n",
    "        loss_func_kwargs = config[\"loss_func_kwargs\"] if \"loss_func_kwargs\" in config.keys() else {}\n",
    "        loss_func = loss_func_class(**loss_func_kwargs)\n",
    "\n",
    "        print(f\"TRAINING MODEL: {model_id}\")\n",
    "\n",
    "        output_model, losses = train_model(\n",
    "            model=model_instance,\n",
    "            loss_func=loss_func,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            epochs=config[\"epoch\"],\n",
    "            lr=config[\"lr\"],\n",
    "            l2_reg=config[\"l2_reg\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            verbose=config[\"verbose\"]\n",
    "        )\n",
    "\n",
    "        models[model_id] = {\"model\": output_model, \"losses\": losses}\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2000\n",
    "LEARNING_RATE = 0.01\n",
    "L2_REGULARIZATION = 0.001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "experiments = {\n",
    "    \"Shallow Model 32 Hidden Nodes\": {\n",
    "        \"model_class\": ShallowModel,\n",
    "        \"model_kwargs\": {\n",
    "            \"num_hidden\": 32\n",
    "        },\n",
    "        \"loss_func_class\": nn.NLLLoss,\n",
    "        \"epoch\": 2000,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"l2_reg\": L2_REGULARIZATION,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    \"Shallow Model 64 Hidden Nodes\": {\n",
    "        \"model_class\": ShallowModel,\n",
    "        \"model_kwargs\": {\n",
    "            \"num_hidden\": 64\n",
    "        },\n",
    "        \"loss_func_class\": nn.NLLLoss,\n",
    "        \"epoch\": 2000,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"l2_reg\": L2_REGULARIZATION,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    \"Shallow Model 128 Hidden Nodes\": {\n",
    "        \"model_class\": ShallowModel,\n",
    "        \"model_kwargs\": {\n",
    "            \"num_hidden\": 128\n",
    "        },\n",
    "        \"loss_func_class\": nn.NLLLoss,\n",
    "        \"epoch\": 2000,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"l2_reg\": L2_REGULARIZATION,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    \"Deep Model 2 Hidden Layers 64 - 32 nodes\": {\n",
    "        \"model_class\": DeepModelTwoHiddenLayer,\n",
    "        \"model_kwargs\": {\n",
    "            \"num_hidden_nodes_1\": 64,\n",
    "            \"num_hidden_nodes_2\": 32\n",
    "        },\n",
    "        \"loss_func_class\": nn.NLLLoss,\n",
    "        \"epoch\": 2000,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"l2_reg\": L2_REGULARIZATION,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    \"Deep Model 2 Hidden Layers 128 - 64 nodes\": {\n",
    "        \"model_class\": DeepModelTwoHiddenLayer,\n",
    "        \"model_kwargs\": {\n",
    "            \"num_hidden_nodes_1\": 128,\n",
    "            \"num_hidden_nodes_2\": 64\n",
    "        },\n",
    "        \"loss_func_class\": nn.NLLLoss,\n",
    "        \"epoch\": 2000,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"l2_reg\": L2_REGULARIZATION,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL: Shallow Model 32 Hidden Nodes\n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.321122  [    0/ 6693]\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.318990  [    0/ 6693]\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.317155  [    0/ 6693]\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.315562  [    0/ 6693]\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.314153  [    0/ 6693]\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.312911  [    0/ 6693]\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.311828  [    0/ 6693]\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.310866  [    0/ 6693]\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.310010  [    0/ 6693]\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.309241  [    0/ 6693]\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.308543  [    0/ 6693]\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.307909  [    0/ 6693]\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.307317  [    0/ 6693]\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.306790  [    0/ 6693]\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.306322  [    0/ 6693]\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.305896  [    0/ 6693]\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.305497  [    0/ 6693]\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.305114  [    0/ 6693]\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.304756  [    0/ 6693]\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.304423  [    0/ 6693]\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.304122  [    0/ 6693]\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.303847  [    0/ 6693]\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.303589  [    0/ 6693]\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.303356  [    0/ 6693]\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.303149  [    0/ 6693]\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302958  [    0/ 6693]\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302780  [    0/ 6693]\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302612  [    0/ 6693]\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302455  [    0/ 6693]\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302306  [    0/ 6693]\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302166  [    0/ 6693]\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.302034  [    0/ 6693]\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301910  [    0/ 6693]\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301793  [    0/ 6693]\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301680  [    0/ 6693]\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301572  [    0/ 6693]\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301470  [    0/ 6693]\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301370  [    0/ 6693]\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301273  [    0/ 6693]\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301183  [    0/ 6693]\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301092  [    0/ 6693]\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.301016  [    0/ 6693]\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300938  [    0/ 6693]\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300857  [    0/ 6693]\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300777  [    0/ 6693]\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300696  [    0/ 6693]\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300617  [    0/ 6693]\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300540  [    0/ 6693]\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300464  [    0/ 6693]\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300389  [    0/ 6693]\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300315  [    0/ 6693]\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300242  [    0/ 6693]\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300170  [    0/ 6693]\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300098  [    0/ 6693]\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.300028  [    0/ 6693]\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299958  [    0/ 6693]\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299890  [    0/ 6693]\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299822  [    0/ 6693]\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299753  [    0/ 6693]\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299685  [    0/ 6693]\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299617  [    0/ 6693]\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299550  [    0/ 6693]\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299483  [    0/ 6693]\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299414  [    0/ 6693]\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299346  [    0/ 6693]\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299278  [    0/ 6693]\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299209  [    0/ 6693]\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299139  [    0/ 6693]\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.299069  [    0/ 6693]\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298999  [    0/ 6693]\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298929  [    0/ 6693]\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298858  [    0/ 6693]\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298787  [    0/ 6693]\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298716  [    0/ 6693]\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298643  [    0/ 6693]\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298572  [    0/ 6693]\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298499  [    0/ 6693]\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298427  [    0/ 6693]\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298354  [    0/ 6693]\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298280  [    0/ 6693]\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298205  [    0/ 6693]\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298129  [    0/ 6693]\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.298049  [    0/ 6693]\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297963  [    0/ 6693]\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297839  [    0/ 6693]\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297735  [    0/ 6693]\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297643  [    0/ 6693]\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297562  [    0/ 6693]\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297481  [    0/ 6693]\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297400  [    0/ 6693]\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297320  [    0/ 6693]\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297238  [    0/ 6693]\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297157  [    0/ 6693]\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.297075  [    0/ 6693]\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296994  [    0/ 6693]\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296911  [    0/ 6693]\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296827  [    0/ 6693]\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296743  [    0/ 6693]\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296658  [    0/ 6693]\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296571  [    0/ 6693]\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296484  [    0/ 6693]\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296396  [    0/ 6693]\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296307  [    0/ 6693]\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296216  [    0/ 6693]\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296125  [    0/ 6693]\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.296033  [    0/ 6693]\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295939  [    0/ 6693]\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295845  [    0/ 6693]\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295749  [    0/ 6693]\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295653  [    0/ 6693]\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295556  [    0/ 6693]\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295458  [    0/ 6693]\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295359  [    0/ 6693]\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295259  [    0/ 6693]\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295158  [    0/ 6693]\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.295056  [    0/ 6693]\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294952  [    0/ 6693]\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294848  [    0/ 6693]\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294743  [    0/ 6693]\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294637  [    0/ 6693]\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294529  [    0/ 6693]\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294420  [    0/ 6693]\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294310  [    0/ 6693]\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294198  [    0/ 6693]\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.294086  [    0/ 6693]\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293972  [    0/ 6693]\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293857  [    0/ 6693]\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293742  [    0/ 6693]\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293625  [    0/ 6693]\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293508  [    0/ 6693]\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293389  [    0/ 6693]\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293268  [    0/ 6693]\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293147  [    0/ 6693]\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.293024  [    0/ 6693]\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292900  [    0/ 6693]\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292775  [    0/ 6693]\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292647  [    0/ 6693]\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292519  [    0/ 6693]\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292389  [    0/ 6693]\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292258  [    0/ 6693]\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.292126  [    0/ 6693]\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291991  [    0/ 6693]\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291855  [    0/ 6693]\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291718  [    0/ 6693]\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291579  [    0/ 6693]\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291439  [    0/ 6693]\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291297  [    0/ 6693]\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291154  [    0/ 6693]\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.291009  [    0/ 6693]\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.290862  [    0/ 6693]\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.290714  [    0/ 6693]\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.290564  [    0/ 6693]\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.290413  [    0/ 6693]\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.290260  [    0/ 6693]\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.290106  [    0/ 6693]\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.289949  [    0/ 6693]\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.289791  [    0/ 6693]\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.289631  [    0/ 6693]\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.289470  [    0/ 6693]\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.289308  [    0/ 6693]\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.289144  [    0/ 6693]\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.288978  [    0/ 6693]\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.288810  [    0/ 6693]\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.288640  [    0/ 6693]\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.288470  [    0/ 6693]\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.288296  [    0/ 6693]\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.288122  [    0/ 6693]\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.287945  [    0/ 6693]\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.287766  [    0/ 6693]\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.287585  [    0/ 6693]\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.287402  [    0/ 6693]\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.287218  [    0/ 6693]\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.287031  [    0/ 6693]\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.286842  [    0/ 6693]\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.286652  [    0/ 6693]\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.286459  [    0/ 6693]\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.286264  [    0/ 6693]\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.286068  [    0/ 6693]\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.285869  [    0/ 6693]\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.285668  [    0/ 6693]\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.285466  [    0/ 6693]\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.285261  [    0/ 6693]\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.285054  [    0/ 6693]\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.284845  [    0/ 6693]\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.284635  [    0/ 6693]\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.284422  [    0/ 6693]\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.284207  [    0/ 6693]\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.283990  [    0/ 6693]\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.283772  [    0/ 6693]\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.283550  [    0/ 6693]\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.283327  [    0/ 6693]\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.283101  [    0/ 6693]\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.282872  [    0/ 6693]\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.282641  [    0/ 6693]\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.282409  [    0/ 6693]\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.282173  [    0/ 6693]\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.281935  [    0/ 6693]\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.281696  [    0/ 6693]\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.281453  [    0/ 6693]\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.281208  [    0/ 6693]\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.280960  [    0/ 6693]\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.280710  [    0/ 6693]\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.280458  [    0/ 6693]\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.280203  [    0/ 6693]\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.279945  [    0/ 6693]\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.279685  [    0/ 6693]\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.279423  [    0/ 6693]\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.279158  [    0/ 6693]\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.278891  [    0/ 6693]\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.278622  [    0/ 6693]\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.278349  [    0/ 6693]\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.278074  [    0/ 6693]\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.277797  [    0/ 6693]\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.277517  [    0/ 6693]\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.277234  [    0/ 6693]\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.276949  [    0/ 6693]\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.276661  [    0/ 6693]\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.276371  [    0/ 6693]\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.276079  [    0/ 6693]\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.275784  [    0/ 6693]\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.275487  [    0/ 6693]\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.275187  [    0/ 6693]\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.274885  [    0/ 6693]\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.274580  [    0/ 6693]\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.274273  [    0/ 6693]\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.273963  [    0/ 6693]\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.273650  [    0/ 6693]\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.273337  [    0/ 6693]\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.273021  [    0/ 6693]\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.272703  [    0/ 6693]\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.272382  [    0/ 6693]\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.272058  [    0/ 6693]\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.271732  [    0/ 6693]\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.271403  [    0/ 6693]\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.271072  [    0/ 6693]\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.270737  [    0/ 6693]\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.270401  [    0/ 6693]\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.270062  [    0/ 6693]\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.269721  [    0/ 6693]\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.269377  [    0/ 6693]\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.269031  [    0/ 6693]\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.268682  [    0/ 6693]\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.268328  [    0/ 6693]\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.267971  [    0/ 6693]\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.267606  [    0/ 6693]\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.267254  [    0/ 6693]\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.266895  [    0/ 6693]\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.266529  [    0/ 6693]\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.266161  [    0/ 6693]\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.265792  [    0/ 6693]\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.265420  [    0/ 6693]\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.265046  [    0/ 6693]\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.264668  [    0/ 6693]\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.264288  [    0/ 6693]\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.263906  [    0/ 6693]\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.263522  [    0/ 6693]\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.263134  [    0/ 6693]\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.262745  [    0/ 6693]\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.262354  [    0/ 6693]\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.261961  [    0/ 6693]\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.261565  [    0/ 6693]\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.261166  [    0/ 6693]\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.260766  [    0/ 6693]\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.260364  [    0/ 6693]\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.259959  [    0/ 6693]\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.259552  [    0/ 6693]\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Batch [000]: loss: 2.259142  [    0/ 6693]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/p82lkpqx5gg9z54ss7lfnq540000gn/T/ipykernel_59217/2396867979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthree_sec_training_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_3_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fg/p82lkpqx5gg9z54ss7lfnq540000gn/T/ipykernel_59217/2186382422.py\u001b[0m in \u001b[0;36mrun_training_experiments\u001b[0;34m(experiment_configs, x_train, y_train)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"TRAINING MODEL: {model_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         output_model, losses = train_model(\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fg/p82lkpqx5gg9z54ss7lfnq540000gn/T/ipykernel_59217/2506627088.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, x_train, y_train, epochs, lr, l2_reg, batch_size, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0;31m# backpropogation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ds4400-final-project/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ds4400-final-project/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "three_sec_training_results = run_training_experiments(experiment_configs=experiments, x_train=x_3_train, y_train=y_3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(outputs):\n",
    "\n",
    "    legend = []\n",
    "    for model_id, output in outputs.items():\n",
    "        legend.append(model_id)\n",
    "        losses = output[\"losses\"]\n",
    "        plt.plot(range(losses), losses)\n",
    "    plt.legend(legend, loc='upper left')\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(outputs=three_sec_training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 2.3030574321746826\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_3_test)\n",
    "after_train = loss_function(y_pred.squeeze(), y_3_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8081627c84a10a0c009821c0f8eb61b714e9fda0899b4e24c997d05071010c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ds4400-final-project': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
