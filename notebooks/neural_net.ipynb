{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4400_final_project.dataset.constants import DATASET_FOLDER\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(csv_filename: str) -> Tuple[np.array, np.array, Dict[int, str], Dict[str, int]]:\n",
    "\t\"\"\" Load the CSV file from the dataset folder. \"\"\"\n",
    "\tfile = str(Path(DATASET_FOLDER) / csv_filename)\n",
    "\tfeatures_list = np.genfromtxt(file, dtype=None, encoding=None, delimiter=\",\", skip_header=1, usecols=range(2, 60))\n",
    "\tfeatures = np.array([list(x) for x in features_list])\n",
    "\n",
    "\t# Create a mapping between a numeric value and genre\n",
    "\tindex_genre_map = {i: genre for i, genre in enumerate(np.unique(features[:,-1]))}\n",
    "\tgenre_index_map = {value: key for key, value in index_genre_map.items()}\n",
    "\n",
    "\t# split the inputs and their labels\n",
    "\tx = normalize(features[:,:57])\n",
    "\t# y = normalize(np.array([genre_index_map[genre] for genre in features[:,-1]]).reshape(-1, 1))\n",
    "\n",
    "\t# take labels and make vector where the value at the index is 1 and everything else is 0\n",
    "\tdef make_one_hot(idx):\n",
    "\t\tv = np.zeros((10,))\n",
    "\t\tv[idx] = 1.0\n",
    "\t\treturn v\n",
    "\t# y = np.array([make_one_hot(genre_index_map[genre]) for genre in features[:,-1]])\n",
    "\ty = np.array([genre_index_map[genre] for genre in features[:,-1]])\n",
    "\n",
    "\treturn x, y, index_genre_map, genre_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data from the 3 seconds and 30 seconds features CSV\n",
    "X_3, y_3, index_genre_map_3, genre_index_map_3 = load_data_from_file(\"features_3_sec.csv\")\n",
    "X_30, y_30, index_genre_map_30, genre_index_map_30 = load_data_from_file(\"features_30_sec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all the data into training and testing sets\n",
    "TEST_SIZE = 0.33\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "x_3_train, x_3_test, y_3_train, y_3_test = train_test_split(X_3, y_3, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "x_30_train, x_30_test, y_30_train, y_30_test = train_test_split(X_30, y_30, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "x_3_train, x_3_test, x_30_train, x_30_test = map(torch.FloatTensor, [x_3_train, x_3_test, x_30_train, x_30_test])\n",
    "y_3_train, y_3_test, y_30_train, y_30_test = map(torch.LongTensor, [y_3_train, y_3_test, y_30_train, y_30_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden: int):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(57, num_hidden)\n",
    "        self.output = nn.Linear(num_hidden, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "        # return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModelTwoHiddenLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden_nodes_1: int, num_hidden_nodes_2: int):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(57, num_hidden_nodes_1)\n",
    "        self.hidden2 = nn.Linear(num_hidden_nodes_1, num_hidden_nodes_2)\n",
    "        self.output = nn.Linear(num_hidden_nodes_2, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        #return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self._x_data = x_data\n",
    "        self._y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return [self._x_data[idx], self._y_data[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "def train_model(model, loss_func, x_train, y_train, epochs: int, lr: float, l2_reg: float, batch_size: int, verbose = False):\n",
    "\tdataset = GTZANDataset(x_train, y_train)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\tsize = len(training_data)\n",
    "\n",
    "\toptimizer = torch.optim.SGD(params=model.parameters(), lr=lr, weight_decay=l2_reg)\n",
    "\n",
    "\t# set the model to training mode\n",
    "\tmodel.train()\n",
    "\n",
    "\t# optimize the model\n",
    "\tfor epoch in range(epochs):\n",
    "\n",
    "\t\tprint(f\"\\nEpoch {epoch+1}\\n-------------------------------\") if verbose else None\n",
    "\n",
    "\t\tfor batch, (x, y) in enumerate(dataloader):\n",
    "\n",
    "\t\t\ty_pred = model(x)\n",
    "\t\t\t# print(y_pred.size())\n",
    "\t\t\t# print(y.size())\n",
    "\n",
    "\t\t\tloss = loss_function(y_pred, y)\n",
    "\n",
    "\t\t\t# backpropogation\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tif verbose and batch % 100 == 0:\n",
    "\t\t\t\t# print(f\"Batch {batch}: train loss: {loss.item()}\")\n",
    "\t\t\t\tloss, current = loss.item(), batch * len(x)\n",
    "\t\t\t\tprint(f\"Batch [{batch:03}]: loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "\t# set the model to evaluation mode\n",
    "\tmodel.eval()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() missing 1 required positional argument: 'loss_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/p82lkpqx5gg9z54ss7lfnq540000gn/T/ipykernel_59217/1882939213.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model_3 = train_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mShallowModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_3_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() missing 1 required positional argument: 'loss_func'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "L2_REGULARIZATION = 0.001\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "model_3 = train_model(\n",
    "    model=ShallowModel(num_hidden=100),\n",
    "    x_train=x_3_train,\n",
    "    y_train=y_3_train,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    l2_reg=L2_REGULARIZATION,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/p82lkpqx5gg9z54ss7lfnq540000gn/T/ipykernel_59217/1491834552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model_deep_one_layer_3 = train_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeepModelTwoHiddenLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden_nodes_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_nodes_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fg/p82lkpqx5gg9z54ss7lfnq540000gn/T/ipykernel_59217/3440786901.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, x_train, y_train, epochs, lr, l2_reg, batch_size, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGTZANDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "L2_REGULARIZATION = 0.001\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "model_deep_one_layer_3 = train_model(\n",
    "    model=DeepModelTwoHiddenLayer(num_hidden_nodes_1=256, num_hidden_nodes_2=128),\n",
    "    loss_func=nn.NLLLoss(),\n",
    "    x_train=x_3_train,\n",
    "    y_train=y_3_train,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    l2_reg=L2_REGULARIZATION,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 2.3030574321746826\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_3_test)\n",
    "after_train = loss_function(y_pred.squeeze(), y_3_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8081627c84a10a0c009821c0f8eb61b714e9fda0899b4e24c997d05071010c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ds4400-final-project': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
